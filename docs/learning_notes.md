# 什麼是 GlobalAveragePooling2D（全局平均池化層）？

全局平均池化層（Global Average Pooling, GAP）是一種深度學習中的特徵壓縮方法，它的作用是在特徵圖的空間維度（Height 和 Width）上計算每個通道的平均值，最終將一個高維的特徵圖壓縮成一個更小的向量。

假設我們的特徵圖是像一張小圖片，每個像素點代表某種「特徵的重要性」，而圖片的通道數（channels）表示我們有多少種不同的特徵。
GlobalAveragePooling2D 就像在每個通道的「整張圖片」上求平均值，從而將每個通道濃縮成一個單一數字。

這個過程的結果是將特徵圖從多維壓縮到僅剩通道數的向量。例如：

* 輸入的特徵圖形狀：(Height, Width, Channels)，例如 (7, 7, 1024)。
* 經過 GlobalAveragePooling2D 後：
  * 輸出形狀變成：(Channels)，例如 (1024)。

| 特性 | GlobalAveragePooling2D | Flatten |
| ---- | ---- | ---- |
|   輸出形狀   |   (channels,)   |   (height × width × channels,)   |
|   計算量   |   少，計算平均值   |   多，需要展平成向量   |
|   參數數量   |  	無    |   通常需要額外的全連接層參數   |
|   適合場景   |   簡化輸出特徵，適合深度學習分類模型   |   需要完整保留特徵的場景   |

## 為什麼要在這裡使用 GlobalAveragePooling2D？

1. 簡化數據，減少參數
   * 卷積層的輸出是多維的特徵圖：
     * 假設輸出特徵圖為 (7, 7, 1024)，這意味著有 1024 個通道，每個通道是一個 7x7 的矩陣。
     * 如果使用全連接層直接處理，需要將這些矩陣展平成一個巨大的向量，參數量會非常龐大。
   * 使用 GAP 可以大幅減少參數數量：
     * GAP 會將每個通道壓縮為一個值，將形狀從 (7, 7, 1024) 壓縮為 (1024)。
     * 這不僅減少了計算量，也降低了過擬合的風險。
2. 自然地保留通道的重要信息
   * GAP 不會丟棄通道信息，只是將每個通道的信息濃縮成了一個值。
   * 與 Flatten() 不同，GAP 更適合處理特徵空間的結構化信息，因為它不直接打亂特徵圖的結構。
3. 避免過多的全連接層
   * 如果不使用 GAP，可能需要額外的全連接層來處理展平的特徵圖（Flatten）。但這會帶來大量的權重參數，增加計算資源需求。
4. 提升模型的泛化能力
   * GAP 的結構簡單，沒有學習參數，能有效防止模型記住訓練集中的噪聲數據（避免過擬合）。
